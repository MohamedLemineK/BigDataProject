{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "!pip install nltk\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# Fonction permettant de rassembler les n mots les plus fréquents d'un fichier texte dans un seul dataframe\n",
    "\n",
    "def mostcommonwords(path,n):\n",
    "    file_location = path\n",
    "    rdd = sc.textFile(file_location)\n",
    "    # On récupère le contenu des fichiers texte\n",
    "    line = rdd.take\n",
    "    lines = rdd.collect()\n",
    "    words = []\n",
    "    # On sépare le texte en lignes\n",
    "    for stuff in lines:\n",
    "        words.append(stuff.split())\n",
    "    \n",
    "    words = list(itertools.chain.from_iterable(words))\n",
    "    # On sépare les lignes en mots pour ensuite créer le dataframe qui contiendra les mots et leur nombre d'occurences\n",
    "\n",
    "    dict = {'word':[],\n",
    "            'occurences':[]}\n",
    "    df_occurences = pd.DataFrame(dict)\n",
    "\n",
    "    # Tout d'abord, on assigne à tous les mots une occurence de 1\n",
    "    for word in words:\n",
    "        df_occurences.loc[len(df_occurences.index)] = [word, 1]\n",
    "\n",
    "    # Par la suite, on retire tous les symboles de ponctuation dans le dataframe \n",
    "    for thing in [*string.punctuation]:\n",
    "        df_occurences.drop(df_occurences[df_occurences['word']==thing].index, inplace = True)\n",
    "\n",
    "    # Enfin, on supprime les mots de liaison tels que 'the', 'an' ou encore 'and'. \n",
    "    # Pour l'instant, nous nous sommes limités aux textes anglais\n",
    "    for bip in stopwords.words(\"english\"):\n",
    "        df_occurences.drop(df_occurences[df_occurences['word']==bip].index, inplace = True)\n",
    "\n",
    "    # Pour obtenir le nombre d'occurence de chaque mot unique, on regroupe les mots identiqus \n",
    "    # en sommant les occurences puis en les classant par ordre décroissant  \n",
    "    df_occurences = df_occurences.groupby(['word'],as_index=False).sum().sort_values(by=['occurences'], ascending=False)\n",
    "\n",
    "    # On ne récupère que les n les plus fréquents\n",
    "    df_occurences_top = df_occurences[:n]\n",
    "\n",
    "    # Enfin on réétablite l'index dont l'ordre a été perturbé par le shuffle and sort\n",
    "    df_occurences_top = df_occurences_top.set_index(pd.Index(range(n)))\n",
    "    \n",
    "    return df_occurences_top\n",
    "\n",
    "df = mostcommonwords('/FileStore/tables/cv998_15691.txt',10)\n",
    "\n",
    "df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique la fonction à 6 fichiers text contenant des critiques de film en gardant le même nom que celui du texte \n",
    "\n",
    "cv994_12270 = mostcommonwords('FileStore/pos/cv994_12270.txt',10)\n",
    "cv995_21821 = mostcommonwords('FileStore/pos/cv995_21821.txt',10)\n",
    "cv996_11592 = mostcommonwords('FileStore/pos/cv996_11592.txt',10)\n",
    "cv997_5046 = mostcommonwords('FileStore/pos/cv997_5046.txt',10)\n",
    "cv998_14111 = mostcommonwords('FileStore/pos/cv998_14111.txt',10)\n",
    "cv999_13106 = mostcommonwords('FileStore/pos/cv999_13106.txt',10)\n",
    "\n",
    "cv997_5046.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Une fois les dataframes créés on peut les rassembler selon les similitudes  \n",
    "\n",
    "dataframes=[cv994_12270,cv995_21821,cv996_11592,cv997_5046,cv998_14111,cv999_13106]\n",
    "names=['cv994_12270','cv995_21821','cv996_11592','cv997_5046','cv998_14111','cv999_13106']\n",
    "# D'abord on assigne aux dataframes un nom correspondant à leur fichier d'origine\n",
    "def nomenclatura(frames, names):\n",
    "    i=0\n",
    "    for df in frames:\n",
    "        df.attrs['name']=names[i]\n",
    "        i+=1\n",
    "    return frames\n",
    "# On rassemble les noms des dataframes en comités. Les membres de chaque comités sont stockés dans un dictionnaire\n",
    "def gathering(frames):\n",
    "    i=1\n",
    "    d={}\n",
    "    for x in range(1,7):\n",
    "        d[\"Committee {0}\".format(x)] = []\n",
    "    for dfe in frames:\n",
    "        d['Committee '+str(i)].append(dfe.attrs['name'])\n",
    "        for dfco in frames:\n",
    "            comp = pd.merge(dfe,dfco,how='inner',on='word')\n",
    "            # Si parmi les 10 mots les plus fréquents 3 sont partagés alors on considère que les textes apartiennent au même comité\n",
    "            if comp.shape[0]<dfco.shape[0] and comp.shape[0]>3:\n",
    "                d['Committee '+str(i)].append(dfco.attrs['name'])        \n",
    "        i+=1\n",
    "    return d\n",
    "\n",
    "dataframes = nomenclatura(dataframes,names)\n",
    "Committees = gathering(dataframes)\n",
    "\n",
    "print(Committees)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une fois les comités établis, on crée des dossiers pour chaque comité où seront stockés les fichiers membres\n",
    "# Dans chaque dossier on copie les fichiers qui lui sont assignés\n",
    "\n",
    "def directorycreation(dictionnary):  \n",
    "    for i in range(1,len(dictionnary)+1):\n",
    "        dbutils.fs.mkdirs(\"/FileStore/Committee\"+str(i))\n",
    "        for frame in dictionnary['Committee '+str(i)]:            \n",
    "            dbutils.fs.cp(\"/FileStore/pos/\"+frame+\".txt\",\"/FileStore/Committee\"+str(i)+\"/\"+frame+\".txt\")   \n",
    "        i+=1\n",
    "    return 0\n",
    "\n",
    "directorycreation(Committees)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs\n",
    "ls /FileStore/Committee4"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
